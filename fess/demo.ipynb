{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning To Rank Demo On Fess\n",
    "  \n",
    "This demo uses data from Fess to demonstrate using [ranklib](https://github.com/codelibs/ranklib) learning to rank models with Elasticsearch.\n",
    "\n",
    "## Setup: Fess\n",
    "\n",
    "This demo requires [Fess 12.2+](https://github.com/codelibs/fess/releases).\n",
    "See [Installation Guide](https://fess.codelibs.org/12.1/install/index.html).\n",
    "\n",
    "1. Launch Fess\n",
    "1. Start Crawler to index documents\n",
    "1. Search some words and then click search results\n",
    "\n",
    "## Download Ranklib\n",
    "\n",
    "The first time you run this demo, fetch RankLib.jar (used to train model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o ranklib.jar http://central.maven.org/maven2/org/codelibs/ranklib/2.10.0/ranklib-2.10.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 0, MART\n",
    "# 1, RankNet\n",
    "# 2, RankBoost\n",
    "# 3, AdaRank\n",
    "# 4, coord Ascent\n",
    "# 6, LambdaMART\n",
    "# 7, ListNET\n",
    "# 8, Random Forests\n",
    "# 9, Linear Regression\n",
    "model_types = [6]\n",
    "# model_types = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "es_host = 'localhost:9201'\n",
    "es_url = f'http://{es_host}'\n",
    "es = Elasticsearch(hosts=es_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_default_store(url='http://localhost:9201', auth=None):\n",
    "    path = f'{url}/_ltr'\n",
    "    resp = requests.delete(path, auth=auth)\n",
    "    print(f\"Delete {path} : {resp.status_code}\")\n",
    "    resp = requests.put(path, auth=auth)\n",
    "    print(f\"Create {path} : {resp.status_code}\")\n",
    "\n",
    "init_default_store(es_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_click_logs(es,\n",
    "                   query_id,\n",
    "                   index_name='fess_log.click_log'):\n",
    "    query = {\"query\": {\"term\": { 'queryId': query_id}}, \"sort\":[\"requestedAt\"]}\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    docs = []\n",
    "    if len(response['hits']['hits']) > 0:\n",
    "        for hit in response['hits']['hits']:\n",
    "            if '_source' in hit:\n",
    "                source = hit.get('_source')\n",
    "                docs.append(source.get('urlId'))\n",
    "    return docs\n",
    "            \n",
    "\n",
    "def process_logs(es,\n",
    "                 judgment_file='fess_judgments.txt',\n",
    "                 index_name = 'fess_log.search_log',\n",
    "                 query = {\"query\": {\"match_all\": {}}, \"sort\":[\"requestedAt\"]},\n",
    "                 num_of_words=1000,\n",
    "                 max_docs=10000):\n",
    "    counter = 0\n",
    "    response = None\n",
    "    running = True\n",
    "    scroll_id = None\n",
    "    word_count = {}\n",
    "    word_doc_count = {}\n",
    "    while(running):\n",
    "        if response is None:\n",
    "            response = es.search(index=index_name,\n",
    "                                 scroll='5m',\n",
    "                                 size=100,\n",
    "                                 body=query,\n",
    "                                 params={\"request_timeout\":60})\n",
    "        else:\n",
    "            response = es.scroll(scroll_id=scroll_id,\n",
    "                                 scroll='5m',\n",
    "                                 params={\"request_timeout\":60})\n",
    "        if len(response['hits']['hits']) == 0:\n",
    "            running = False\n",
    "            break\n",
    "        scroll_id = response['_scroll_id']\n",
    "        for hit in response['hits']['hits']:\n",
    "            if '_source' in hit:\n",
    "                counter += 1\n",
    "                if counter % 1000 == 0:\n",
    "                    print(\"%(timestamp)s %(author)s: %(text)s\" % hit[\"_source\"])\n",
    "                source = hit.get('_source')\n",
    "                if 'documents' in source:\n",
    "                    search_word = source.get('searchWord').lower()\n",
    "                    query_id = source.get('queryId')\n",
    "                    doc_ids = get_click_logs(es, query_id)\n",
    "                    if search_word in word_count:\n",
    "                        word_count[search_word] = word_count.get(search_word) + 1\n",
    "                        doc_count = word_doc_count.get(search_word)\n",
    "                    else:\n",
    "                        word_count[search_word] = 1\n",
    "                        doc_count = {}\n",
    "                        word_doc_count[search_word] = doc_count\n",
    "                    for doc in source.get('documents'):\n",
    "                        doc_id = doc.get('_id')\n",
    "                        if doc_id is not None:\n",
    "                            if doc_id in doc_ids: # clicked\n",
    "                                if doc_id in doc_count:\n",
    "                                    doc_count[doc_id] = doc_count[doc_id] + 1\n",
    "                                else:\n",
    "                                    doc_count[doc_id] = 1\n",
    "                            else:\n",
    "                                if doc_id not in doc_count:\n",
    "                                    doc_count[doc_id] = 0\n",
    "\n",
    "    with open(judgment_file, 'wt') as f:\n",
    "        qid_list = []\n",
    "        for i, (k, v) in enumerate(sorted(word_count.items(), key=lambda x: -x[1])):\n",
    "            if i > num_of_words:\n",
    "                break\n",
    "            doc_count = word_doc_count.get(k)\n",
    "#             print(f'{k}: {np.array(list(doc_count.values()))}')\n",
    "            if np.array(list(doc_count.values())).sum() == 0:\n",
    "                continue\n",
    "            qid = len(qid_list) + 1\n",
    "            f.write(f'# qid:{qid}: {k}\\n')\n",
    "            qid_list.append((qid, k))\n",
    "\n",
    "        for qid, k in qid_list:\n",
    "            if i > num_of_words:\n",
    "                break\n",
    "            doc_count = word_doc_count.get(k)\n",
    "            thresholds = np.percentile(list(doc_count.values()), [25, 50, 75])\n",
    "            \n",
    "            def get_rank(v):\n",
    "                if v == 0:\n",
    "                    return 0\n",
    "                elif v < thresholds[0]:\n",
    "                    return 1\n",
    "                elif v < thresholds[1]:\n",
    "                    return 2\n",
    "                elif v < thresholds[2]:\n",
    "                    return 3\n",
    "                else:\n",
    "                    return 4\n",
    "\n",
    "            rank0_count = int(len(doc_count)*0.3) # 30%\n",
    "            for x, y in doc_count.items():\n",
    "                if y == 0:\n",
    "                    if rank0_count == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        rank0_count -= 1\n",
    "                f.write(f'{get_rank(y)} qid:{qid} # {x} {k}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create fess_judgments.txt\n",
    "process_logs(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature(ftr_id):\n",
    "    with open(f'features/{ftr_id}.json', 'rt') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def each_feature():\n",
    "    try:\n",
    "        ftr_id = 1\n",
    "        while True:\n",
    "            parsed_json = get_feature(ftr_id)\n",
    "            print(f'Load Feature{ftr_id}')\n",
    "            template = parsed_json['query']\n",
    "            feature_spec = {\n",
    "                \"name\": str(ftr_id),\n",
    "                \"params\": [\"query\"],\n",
    "                \"template\": template\n",
    "            }\n",
    "            yield feature_spec\n",
    "            ftr_id += 1\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "def load_features(url='http://localhost:9201',\n",
    "                  feature_set_name='fess_features',\n",
    "                  auth=None):\n",
    "    feature_set = {\n",
    "        \"featureset\": {\n",
    "            \"name\": feature_set_name,\n",
    "            \"features\": [feature for feature in each_feature()]\n",
    "        }\n",
    "    }\n",
    "    path = f\"_ltr/_featureset/{feature_set_name}\"\n",
    "    full_path = f'{url}/{path}'\n",
    "    head = {'Content-Type': 'application/json'}\n",
    "    resp = requests.post(full_path, data=json.dumps(feature_set), headers=head, auth=auth)\n",
    "    print(f'Send {full_path} : {resp.status_code}')\n",
    "\n",
    "load_features(es_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Judgment:\n",
    "    def __init__(self, grade, qid, keywords, doc_id):\n",
    "        self.grade = grade\n",
    "        self.qid = qid\n",
    "        self.keywords = keywords\n",
    "        self.doc_id = doc_id\n",
    "        self.features = [] # 0th feature is ranklib feature 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"grade:{self.grade} qid:{self.qid} ({self.keyword}) docid:{self.doc_id}\"\n",
    "\n",
    "    def to_ranklib_format(self):\n",
    "        features_as_strs = \"\\t\".join([f'{idx+1}:{feature}' for idx, feature in enumerate(self.features)])\n",
    "        comment = f\"# {self.doc_id}\\t{self.keywords}\"\n",
    "        return f\"{self.grade}\\tqid:{self.qid}\\t{features_as_strs} {comment}\"\n",
    "\n",
    "    \n",
    "def queries_from_header(lines):\n",
    "    \"\"\" Parses out mapping between, query id and user keywords\n",
    "        from header comments, ie:\n",
    "        # qid:523: First Blood\n",
    "        returns dict mapping all query ids to search keywords\"\"\"\n",
    "    regex = re.compile('#\\sqid:(\\d+?):\\s+?(.*)')\n",
    "    values = {}\n",
    "    for line in lines:\n",
    "        if line[0] != '#':\n",
    "            break\n",
    "        m = re.match(regex, line)\n",
    "        if m:\n",
    "            values[int(m.group(1))] = m.group(2)\n",
    "\n",
    "    return values\n",
    "\n",
    "def judgments_from_body(lines):\n",
    "    \"\"\" Parses out judgment/grade, query id, and docId in line such as:\n",
    "         4  qid:523   # a01  Grade for Rambo for query Foo\n",
    "        <judgment> qid:<queryid> # docId <rest of comment ignored...)\"\"\"\n",
    "    regex = re.compile('^(\\d)\\s+qid:(\\d+)\\s+#\\s+(\\S+).*')\n",
    "    for line in lines:\n",
    "#         print(line)\n",
    "        m = re.match(regex, line)\n",
    "        if m:\n",
    "#             print(\"%s,%s,%s\" % (m.group(1), m.group(2), m.group(3)))\n",
    "            yield int(m.group(1)), int(m.group(2)), m.group(3)\n",
    "\n",
    "def judgments_from_file(filename):\n",
    "    with open(filename) as f:\n",
    "        qid2keywords = queries_from_header(f)\n",
    "    with open(filename) as f:\n",
    "        for grade, qid, doc_id in judgments_from_body(f):\n",
    "            yield Judgment(grade=grade, qid=qid, keywords=qid2keywords[qid], doc_id=doc_id)\n",
    "\n",
    "judgments = judgments_from_file(filename='fess_judgments.txt')\n",
    "# judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgments_by_qid(judgments):\n",
    "    values = {}\n",
    "    for judgment in judgments:\n",
    "        try:\n",
    "            values[judgment.qid].append(judgment)\n",
    "        except KeyError:\n",
    "            values[judgment.qid] = [judgment]\n",
    "    return values\n",
    "\n",
    "\n",
    "fess_judgments = judgments_by_qid(judgments)\n",
    "# fess_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_query = {  \n",
    "    \"size\":10000,\n",
    "    \"query\":{  \n",
    "        \"bool\":{  \n",
    "            \"filter\":[  \n",
    "                {  \n",
    "                    \"ids\":{  \n",
    "                        \"values\":[\"7555\"]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\":[  \n",
    "                {  \n",
    "                    \"sltr\":{  \n",
    "                        \"_name\":\"logged_featureset\",\n",
    "                        \"featureset\":\"movie_features\",\n",
    "                        \"params\":{  \n",
    "                            \"query\":\"rambo\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"ext\":{  \n",
    "        \"ltr_log\":{  \n",
    "            \"log_specs\":{  \n",
    "                \"name\":\"main\",\n",
    "                \"named_query\":\"logged_featureset\",\n",
    "                \"missing_as_zero\":True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def feature_dict_to_list(ranklib_labeled_features):\n",
    "    values = [0.0] * len(ranklib_labeled_features)\n",
    "    for idx, log_entry in enumerate(ranklib_labeled_features):\n",
    "        value = log_entry['value']\n",
    "        try:\n",
    "            values[idx] = value\n",
    "        except IndexError:\n",
    "            print(f\"Out of range {idx}\")\n",
    "    return values\n",
    "\n",
    "\n",
    "def log_features(es, judgments_by_qid, \n",
    "                 index_name='fess.search',\n",
    "                 feature_set_name='fess_features'):\n",
    "    for qid, judgments in judgments_by_qid.items():\n",
    "        keywords = judgments[0].keywords\n",
    "        doc_ids = [judgment.doc_id for judgment in judgments]\n",
    "#         print(f'{doc_ids}')\n",
    "        log_query['query']['bool']['filter'][0]['ids']['values'] = doc_ids\n",
    "        log_query['query']['bool']['should'][0]['sltr']['featureset'] = feature_set_name\n",
    "        log_query['query']['bool']['should'][0]['sltr']['params']['query'] = keywords\n",
    "#         print(json.dumps(log_query, indent=2))\n",
    "        res = es.search(index=index_name, body=log_query)\n",
    "        # Add feature back to each judgment\n",
    "        features_per_doc = {}\n",
    "#         print(f'{res}')\n",
    "        for doc in res['hits']['hits']:\n",
    "#             print(f'{doc}')\n",
    "            doc_id = doc['_id']\n",
    "            features = doc['fields']['_ltrlog'][0]['main']\n",
    "            features_per_doc[doc_id] = feature_dict_to_list(features)\n",
    "\n",
    "        # Append features from ES back to ranklib judgment list\n",
    "        for judgment in judgments:\n",
    "            try:\n",
    "                features = features_per_doc[judgment.doc_id]\n",
    "                judgment.features = features\n",
    "            except KeyError:\n",
    "                print(f\"Missing doc: {judgment.doc_id}\")\n",
    "\n",
    "log_features(es, judgments_by_qid=fess_judgments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_judgments_file(judgments_with_features, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        for qid, judgment_list in judgments_with_features.items():\n",
    "            for judgment in judgment_list:\n",
    "                f.write(judgment.to_ranklib_format() + \"\\n\")\n",
    "\n",
    "\n",
    "build_features_judgments_file(fess_judgments, filename='fess_features.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(features_file,\n",
    "                model_file,\n",
    "                which_model=6):\n",
    "    cmd = ['java',\n",
    "           \"-Djava.util.logging.SimpleFormatter.format='%5$s%n'\",\n",
    "           '-jar', 'ranklib.jar',\n",
    "           '-ranker', f'{which_model}',\n",
    "           '-train', f'{features_file}',\n",
    "           '-save', f'{model_file}',\n",
    "           '-frate', '1.0']\n",
    "    print(f\"Running {' '.join(cmd)}\")\n",
    "    os.system(' '.join(cmd))\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Training {model_type}\")\n",
    "    model_file = f'model_{model_type}.txt'\n",
    "    train_model(features_file='fess_features.txt',\n",
    "                model_file=model_file,\n",
    "                which_model=model_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(script_name, feature_set, model_file,\n",
    "               url='http://localhost:9201',\n",
    "               auth=None):\n",
    "    \"\"\" Save the ranklib model in Elasticsearch \"\"\"\n",
    "    model_payload = {\n",
    "        \"model\": {\n",
    "            \"name\": script_name,\n",
    "            \"model\": {\n",
    "                \"type\": \"model/ranklib\",\n",
    "                \"definition\": {\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    path = f'{url}/_ltr/_model/{script_name}'\n",
    "    resp = requests.delete(path, auth=auth)\n",
    "    print(f\"Delete {path} : {resp.status_code}\")\n",
    "\n",
    "    model_data = []\n",
    "    with open(model_file, 'rt') as mf:\n",
    "        for line in mf.readlines():\n",
    "            line = line.strip()\n",
    "            model_data.append(line)\n",
    "\n",
    "    path = f\"{url}/_ltr/_featureset/{feature_set}/_createmodel\"\n",
    "    model_payload['model']['model']['definition'] = '\\n'.join(model_data)\n",
    "    head = {'Content-Type': 'application/json'}\n",
    "    resp = requests.post(path, data=json.dumps(model_payload), headers=head, auth=auth)\n",
    "    print(f'Send {path} : {resp.status_code}')\n",
    "    if (resp.status_code >= 300):\n",
    "        print(f'{resp.text}')\n",
    "\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"Deploying {model_type}\")\n",
    "    model_file = f'model_{model_type}.txt'\n",
    "    save_model(script_name=f\"model_{model_type}\",\n",
    "               feature_set='fess_features',\n",
    "               model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
